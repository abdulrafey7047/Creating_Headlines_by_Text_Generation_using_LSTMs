# Headline Generation Using LSTMs

In this project a basic `LSTM-DNN` model is trained on [New York Times Comments](https://www.kaggle.com/datasets/aashita/nyt-comments) dataset to generate headlines. Although the dataset has a lot of features but we only use the `healdine` column to train our model.

## Tools Used
- [Tensorflow](https://www.tensorflow.org/) - For Deep Learning libraries
- [Keras](https://keras.io/) - To create the model
- [Pandas](https://pandas.pydata.org/) and [Numpy](https://numpy.org/) - To pre-process the data

## Techniques
 - Data Cleaning by removing punctuations.
 - Prepending all headlines with `<START>` tag and appending with `<END>` tag.
 - Generating `n-grams` for all headlines.
 - `Tokenizing` and `Padding` all sequences of `n-grams`.
 - Using Stanford's 50-dimensional [GloVe](https://nlp.stanford.edu/projects/glove/) `Embeddings` which are trained on 6 billion tokens.

## Model Architecture
 - The model has one `Embedding` layer, followed by two stacked `LSTM` layers and lastly two `Dense` layers. `LSTM` layers and `Dense` layers are separated by `Dropout` layers.
- `Adam` was used as the optimizer and `Categorical Cross Entropy` was used as the loss function.
- Model was trained for `100 epochs` with a `learning rate` of `0.001`.

## Usage
All of the code is present in [main.ipynb](https://github.com/abdulrafey7047/Creating_Headlines_by_Text_Generation_using_LSTMs/blob/main/main.ipynb). In order to run it you must first install the dependencies using pip. (if you don't have pip read [this](https://www.geeksforgeeks.org/how-to-install-pip-on-windows/))

```
pip install -r requirements.txt
```

After installing dependencies, the code should run fine.

## Headline Generation
The `generate_healdine` function in [main.ipynb](https://github.com/abdulrafey7047/Creating_Headlines_by_Text_Generation_using_LSTMs/blob/main/main.ipynb) is a wrapper aorund the model that post-processes the model's results to generate a text based headline.

An `inital sentence` can be passed to the model and the model will start generating a headline form that sentence, if no `initial sentence` is passed then the `<START>` token is used as the `initial sentence`.

The model predicts one word at a time, for generating the next word, the previous generated word is appended to the `generated sentence` and that `generated sentence` is now passed to the model instead of `inital sentence`. The model keeps on generating words until the headline reaches a `max_sentence_length` (specified by the user) or the model generates the `<END>` token.

To generate multiple different headlines for the same `inital sentence`, `generate_healdine` function randomly samples one word from the top `word_sample_size` (specified by the user) words generated by the model. 



## Results
- Passing only the `<START>` token as initail sentence:
![Result1](https://github.com/abdulrafey7047/Creating_Headlines_by_Text_Generation_using_LSTMs/blob/main/results/result1.png?raw=true)
- Passing `<START> new york` as the inital sentence:
![Result2](https://github.com/abdulrafey7047/Creating_Headlines_by_Text_Generation_using_LSTMs/blob/main/results/result2.png?raw=true)
![Result3](https://github.com/abdulrafey7047/Creating_Headlines_by_Text_Generation_using_LSTMs/blob/main/results/result3.png?raw=true)


## Future Improvements
The model generates short headlines well. But, has difficulty in generating longer headlines. Some improvements that can be made in the future are:

- The model architecture is very shallow. `More layers can added to make the model deep`. A deeper models is better at capturing context.
- The training time can be increased by `increasing the number of epochs`
- `More traning data` can be used, with longer headlines.
- A `character level model and embeddings` can be introduced. The reults of both models, character level and word level, can be used in conjuction by the last `Dense` layers to classify the next word.

