{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout\n",
    "from tensorflow import one_hot\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "\n",
    "## To replicate the results\n",
    "from tensorflow.random import set_seed\n",
    "from numpy.random import seed\n",
    "\n",
    "set_seed(42)\n",
    "seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_grams(sentence):\n",
    "\n",
    "    n_grams = []\n",
    "    sentence_words = sentence.split()\n",
    "    for i in range(2, len(sentence_words) + 1):\n",
    "        n_grams.append(' '.join(sentence_words[0: i]))\n",
    "\n",
    "    return n_grams\n",
    "\n",
    "\n",
    "def generate_headline(start, max_sentence_length, sequence_len, tokenizer, model, word_sample_size):\n",
    "\n",
    "    generated_sentence = start.split()\n",
    "    for i in range (max_sentence_length-2):\n",
    "\n",
    "        generated_sentence_tokens = tokenizer.texts_to_sequences([generated_sentence])\n",
    "        generated_sentence_padded_tokens = pad_sequences(generated_sentence_tokens, maxlen=sequence_len)\n",
    "\n",
    "        pred_tokens = model.predict(generated_sentence_padded_tokens)[0]\n",
    "        top_n_pred_tokens = np.argpartition(pred_tokens, -word_sample_size)[-word_sample_size:]\n",
    "        pred_token = np.random.choice(top_n_pred_tokens, size=1)\n",
    "\n",
    "        pred_text = tokenizer.sequences_to_texts([pred_token])[0]\n",
    "        generated_sentence.append(pred_text)\n",
    "        if pred_text == '<END>':\n",
    "            return ' '.join(generated_sentence)\n",
    "\n",
    "\n",
    "    generated_sentence.append('<END>')\n",
    "    return ' '.join(generated_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = np.array([])\n",
    "\n",
    "dataset_dir = 'dataset/'\n",
    "for filename in os.listdir(dataset_dir):\n",
    "    filepath = dataset_dir + filename\n",
    "    if 'Article' in filename:\n",
    "        headlines = np.append(headlines, pd.read_csv(filepath).headline.values)\n",
    "\n",
    "f'Extracted a total of {headlines.shape[0]} headlines from the dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = pd.Series(headlines)\n",
    "\n",
    "## Converting to Lowercase\n",
    "headlines = headlines.apply(str.lower)\n",
    "\n",
    "## Removing Punctuations\n",
    "headlines = headlines.apply(\n",
    "    lambda headline: re.sub(r'[^\\w\\s]', '', headline))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding &lt;SRART&gt; and &lt;END&gt; tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = headlines.apply(\n",
    "    lambda headline: f'<START> {headline} <END>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating n grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "headline_ngrams = np.array([])\n",
    "\n",
    "for headline in tqdm(headlines):\n",
    "    n_grams = generate_n_grams(headline)\n",
    "    vocab = vocab.union(set(n_grams[-1].split()))\n",
    "    # headline_ngrams.append(generate_n_grams(headline))\n",
    "    headline_ngrams = np.append(headline_ngrams, generate_n_grams(headline))\n",
    "\n",
    "f'There are {len(vocab)} words in the dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='<OOV>', filters=[], lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_headline_ngrams = tokenizer.texts_to_sequences(headline_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_headline_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking length of headlines in dataset and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_lengths = headlines.map(str.split).map(len)\n",
    "\n",
    "print(f'''\n",
    "    Maximum Headline length: {headline_lengths.max()}\n",
    "    Minimum Headline length: {headline_lengths.min()}\n",
    "    Average Headline length: {headline_lengths.mean():.2f}\n",
    "    STD of Headline length: {headline_lengths.std():.2f}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since them longest headline is not too long e.g 150 or 200, we can use the length of longest headline for padding\n",
    "#### If it was around 150 or 200 then we would truncate the longer sentences and use a padding length of a smaller value e.g mean_length + (2 * std of length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_tokenized_headline_ngrams = pad_sequences(tokenized_headline_ngrams, maxlen=headline_lengths.max(), padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_tokenized_headline_ngrams[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_tokenized_headline_ngrams[:, :-1]\n",
    "y = padded_tokenized_headline_ngrams[:, -1]\n",
    "\n",
    "y = one_hot(y, depth=len(vocab) + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab) + 2\n",
    "embedding_dim = 50\n",
    "sequence_len = headline_lengths.max() - 1\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "with open('glove.6B/glove.6B.50d.txt') as f:\n",
    "    for line in f:\n",
    "        word, embedding = line.split(maxsplit=1)\n",
    "        embedding = np.fromstring(embedding, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=sequence_len,\n",
    "        embeddings_initializer=Constant(embedding_matrix),\n",
    "        trainable=False\n",
    "    ),\n",
    "    LSTM(units=sequence_len, return_sequences=True),\n",
    "    Dropout(rate=dropout_rate),\n",
    "    LSTM(units=sequence_len),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dropout(rate=dropout_rate),\n",
    "    Dense(units=vocab_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('headline_generating_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('headline_generating_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_headline('<START>', 15, sequence_len, tokenizer, model, word_sample_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_headline('<START> new', 15, sequence_len, tokenizer, model, word_sample_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_headline('<START> new york', 10, sequence_len, tokenizer, model, word_sample_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_headline('<START> new york', 10, sequence_len, tokenizer, model, word_sample_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('data-science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3bfd8a20af1d5d4753b5a38df58ddc27d816f3a7f519b02fcfafca237454594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
